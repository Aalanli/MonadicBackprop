#![allow(non_snake_case)]
#![allow(dead_code)]
use std::marker::PhantomData;
use std::{ops::{Add, AddAssign}, sync::Arc};

/// There are three methods I can think of for effectively implementing backprop in rust, and indeed generalizing it.
/// 
/// The first is perhaps the most manual, but probably the most efficient.
/// Every function accepts arbitrary arguments and returns a tuple, where the first position is arbitrary, and the second
/// is the derivative function. So functions in the form of this:
fn add(x: f32, y: f32) -> (f32, impl Fn(f32) -> (f32, f32)) {
    (x + y, |grad| (grad, grad))
}

fn mul(x: f32, y: f32) -> (f32, impl Fn(f32) -> (f32, f32)) {
    (x * y, move |grad| (grad * y, grad * x))
}

/// The composition of functions in this form need only support addition. While this can be done manually, a proc-macro could
/// potentially automate this.
fn compose(x: f32) -> ((f32, f32, f32), impl Fn(f32, f32, f32) -> f32) {
    let (y1, f1) = add(x, 1.0);
    let (y2, f2) = mul(x, y1);
    let (y3, f3) = mul(y1, y2);
    let result = (y1, y2, y3);
    let f4 = move |g1: f32, g2: f32, g3: f32| -> f32 {
        let (dy13, dy23) = f3(g3);
        let (dx2, dy12) = f2(g2 + dy23);
        let (dx1, _) = f1(g1 + dy12 + dy13);
        dx1 + dx2
    };
    (result, f4)
}

/// Some subtleties can arise when the objects in question are not copy. As in it would be expensive to clone them.
use ndarray::prelude::*;

/// One could simply be naive, and clone the objects.
fn mul_array<D: Dimension>(a: &Array<f64, D>, b: f64) -> (Array<f64, D>, impl FnOnce(&Array<f64, D>) -> (Array<f64, D>, f64)) {
    let out = a * b;
    let a_cloned = a.clone();
    let f = move |grad: &Array<f64, D>| {
        (grad * b, (grad * a_cloned).sum())
    };
    (out, f)
}

/// Or use a shared pointer. This would be the easiest for implementing the proc macro, but I am unsure if the performance cost is worth it
/// for many small arrays.
fn mul_array_arc<D: Dimension>(a: &Arc<Array<f64, D>>, b: f64) -> (Array<f64, D>, impl Fn(&Array<f64, D>) -> (Array<f64, D>, f64)) {
    let a_ref: &Array<f64, D> = &*a;

    let out = a_ref * b;
    let a_cloned = a.clone();
    let f = move |grad: &Array<f64, D>| {
        (grad * b, (grad * &*a_cloned).sum())
    };
    (out, f)
}

/// Or use lifetimes. This structure would make it difficult to implement an auto deriving proc macro. Since type information is not available,
/// one would need to annotate when the lifetime of the backwards function and the argument is linked, for the auto generated backwards pass to
/// move the right things correctly into the closure. However, this version is probably the most efficient in terms of memory and instructions.
fn mul_array2<'a, D: Dimension>(a: &'a Array<f64, D>, b: f64) -> (Array<f64, D>, impl FnOnce(&Array<f64, D>) -> (Array<f64, D>, f64) + 'a) {
    let out = a * b;
    let f = move |grad: &Array<f64, D>| {
        (grad * b, (grad * a).sum())
    };
    (out, f)
}

/// Overall, the manual method, while general in terms of the arguments it accepts, is not so flexible in terms of other ml related tasks.
/// Such as inspecting and debuging the backwards pass, as realistically, most of the backwards passes would need to be generated by a proc macro,
/// which would be the bottleneck in this setup.
/// 
/// For this to work, I am thinking of at least 3 constructs the macro would need to support.
/// 1. f(a: Require<T>), annotating that the lifetime of the backwards closure is linked to this argument
/// 2. either f(a: Ignore<T>), or f(a: Grad<T>), either whitelist the gradient parameters, or backlist the non-gradient parameters
///     of course, all Require<T> is also Grad<T>
/// 3. A 'nograd {...} block
/// 
/// Optional: Every function accepts a ctx parameter as the first argument, which is elided in the macro.
/// 
/// There is yet another complication in the case for structs. We would like to support a fully functional style.
/// So there would need to be a dual
/// This would be defined by the user, who would annotate a derive
/// #[derive(Grad)]
struct Convolve {
    weight: ArrayParam<Array4<f32>>, // to annotate that this needs to be generated
    // or annotate via an attribute: 
    // #[grad]
    bias: ArrayParam<Array1<f32>>,
    stride: [u32; 2],
    padding: [u32; 2]
}
/// This would be auto generated.
struct ConvolveGrad {
    d_weight: Array4<f32>,
    d_bias: Array1<f32>,
}

struct ArrayParam<T>(T);

fn convolve<'a>(conv: &'a Convolve, x: &'a Array4<f32>) -> (Array4<f32>, impl FnOnce(&Array4<f32>) -> ConvolveGrad + 'a) {
    (x.clone(), |grad| ConvolveGrad { d_weight: grad.clone(), d_bias: conv.bias.0.clone() } )
}

/// Since each weight may have multiple dependencies, to collect gradients properly, the dual grad struct would need to support a notion of addition
/// This would be auto-generated
impl Add<ConvolveGrad> for ConvolveGrad {
    type Output = ConvolveGrad;
    fn add(self, rhs: ConvolveGrad) -> Self::Output {
        ConvolveGrad { d_weight: self.d_weight + rhs.d_weight, d_bias: self.d_bias + rhs.d_bias }
    }
}

impl AddAssign for ConvolveGrad {
    fn add_assign(&mut self, rhs: Self) {
        self.d_weight += &rhs.d_weight;
        self.d_bias += &rhs.d_bias;
    }
}

/// Updating parameters is tricky, if the elements are of the same type, then we could simply auto generate an 'Unroller' for each
/// weight, grad pair. Otherwise we may need to do something more.
use std::iter::Extend;

trait Unroll<'a, Out> {
    type Grad;
    fn unroll(&'a mut self, grad: Self::Grad) -> Out;
}

/// This would probably need to be implemented for each unroller
impl<'a, D: Dimension + 'a, F: num::Float + 'a> Unroll<'a, (&'a mut [F], &'a [F])> for ArrayParam<Array<F, D>> {
    type Grad = &'a Array<F, D>;
    fn unroll(&'a mut self, grad: Self::Grad) -> (&mut [F], &[F]) {
        (self.0.as_slice_mut().unwrap(), grad.as_slice().unwrap())
    }
}

impl<'a> Unroll<'a, Vec<(&'a mut [f32], &'a [f32])>> for Convolve {
    type Grad = &'a ConvolveGrad;
    fn unroll(&'a mut self, grad: Self::Grad) -> Vec<(&'a mut [f32], &'a [f32])> {
        let a = self.weight.unroll(&grad.d_weight);
        let b = self.bias.unroll(&grad.d_bias);
        vec![a, b]
    }
}

/// To be more parametric in the unroller, more complication is needed, in case the optimizer requires more
/// information than just a flattened slice
struct Linear<UnrollT> {
    weight: ArrayParam<Array2<f32>>,
    _unroll_t: PhantomData<UnrollT>
}

struct LinearGrad {
    d_weight: Array2<f32>
}

impl<'a, UnrollT> Unroll<'a, Vec<UnrollT>> for Linear<UnrollT> 
where ArrayParam<Array2<f32>>: Unroll<'a, UnrollT, Grad = &'a Array2<f32>> {
    type Grad = &'a LinearGrad;
    fn unroll(&'a mut self, grad: Self::Grad) -> Vec<UnrollT> {
        vec![self.weight.unroll(&grad.d_weight)]
    }
}
/// This is very unfortunate, it seems that to support arbitrary types in the optimizer, we need
/// to perform type-foo every time a new model is created. 


/// This is slightly better, but still convoluted.
/// 
/// We let each param be packed with an 'Updater'
struct Param<T, U> {
    x: T,
    updater: U
}

impl<T, U: Update<T, G = T>> Param<T, U> {
    fn update_param(&mut self, grad: &T) {
        self.updater.update(&mut self.x, grad);
    }
}

/// This would be the 'Updater'
struct Adam<T> {
    beta1: f32,
    beta2: f32,
    mt: T,
    vt: T
}

trait Update<W> {
    type G;
    fn update<'a>(&mut self, x: &'a mut W, grad: &'a Self::G);
}

impl<D: Dimension> Update<Array<f32, D>> for Adam<Array<f32, D>> {
    type G = Array<f32, D>;
    fn update<'a>(&mut self, _x: &'a mut Array<f32, D>, _grad: &'a Self::G) {
        todo!()
    }
}

struct Linear2<U> {
    weight: Param<Array2<f32>, U>,
}

struct Linear2Grad {
    d_weight: Array2<f32>
}

/// now this Updater trait duplicates the Update trait, which is unsavory, but higher, user defined
/// structs would implement this trait, or for it to be auto generated.
trait Updater {
    type Grad;
    fn update<'a>(&mut self, grad: &'a Self::Grad);
}

impl<U: Update<Array2<f32>, G = Array2<f32>>> Updater for Linear2<U> {
    type Grad = Linear2Grad;
    fn update<'a>(&mut self, grad: &'a Self::Grad) {
        self.weight.update_param(&grad.d_weight);
    }
}

/// Or one may take inspiration from ecs, and derive a Flatten trait and Query trait, for each user defined struct.
/// Then, one can dynamically inspect the weights using typed queries (see below), to avoid the previous nightmares.


/// All of the above seems like quite a hassle for something so simple. The type-manipulation is cognitively demanding,
/// while not actually solving the problem at hand. 
/// 
/// Perhaps a generalization could be made, that there are two parts to the program, computing the results and gradients with
/// weights, and later processing the computed results.
/// 
/// Perhaps we could flatten the second part, while making the first part slightly more complicated.

trait Ctx<Store> {
    type Id;
    fn store_weight(&mut self, s: Store) -> Self::Id;
    fn store_grad(&mut self, s: Store) -> Self::Id;
    fn store_temp(&mut self, s: Store) -> Self::Id;
    fn get(&self, id: Self::Id) -> &Store;
}

#[derive(Copy, Clone)]
struct ArrayId(usize);
#[derive(Copy, Clone)]
struct StaticArrayId<D>(usize, D);


fn mul_ctx<D, F, C>(ctx: &mut C, a: StaticArrayId<D>, b: StaticArrayId<D>) 
    -> (StaticArrayId<D>, impl FnMut(&mut C, StaticArrayId<D>) -> (StaticArrayId<D>, StaticArrayId<D>)) 
    where D: Dimension + Copy, F: num::Float, C: Ctx<Array<F, D>, Id = StaticArrayId<D>> 
{
    let aref = ctx.get(a);
    let bref = ctx.get(b);

    let result = aref * bref;
    let id = ctx.store_weight(result);

    let f = move |ctx: &mut C, grad_id| {
        let aref = ctx.get(a);
        let bref = ctx.get(b);
        let gref = ctx.get(grad_id);

        let da = gref * bref;
        let db = gref * aref;
        (ctx.store_grad(da), ctx.store_grad(db))
    };

    (id, f)
}

/// Or perhaps with less trait and type-foo, user-defined structs only store ids, while there is a ctx
/// that is passed around, which stores arrays.
/// 
/// We could do an ecs style ctx, where Entites are light copible ids, components are Arrays and associated information
/// then we query them with get<Query>(id)
/// 
/// use std::any::TypeId to make this operation safe


trait Ctx2<Store> {
    type Id;
    type Shape;
    fn get(&self, id: Self::Id) -> &Store;
    fn alloc(&self, shape: Self::Shape) -> (Self::Id, &mut Store);
    fn alloc_temp(&self, shape: Self::Shape) -> &mut Store; // the temp buffer would be reset after every function call
    fn reset_temp(&self);
}

fn mul_ctx_help<D, F, C>(ctx: &mut C, a: StaticArrayId<D>, b: StaticArrayId<D>) -> StaticArrayId<D>
where D: Dimension + Copy, F: num::Float, C: Ctx2<Array<F, D>, Id = StaticArrayId<D>, Shape = <D as Dimension>::Pattern> {
    let aref = ctx.get(a);
    let bref = ctx.get(b);

    let (id, result) = ctx.alloc(aref.dim());
    result.iter_mut().zip(aref.iter()).zip(bref.iter()).for_each(|((y, a), b)| {
        *y = *a * *b;
    });

    id
}

fn mul_ctx2<D, F, C>(ctx: &mut C, a: StaticArrayId<D>, b: StaticArrayId<D>) 
    -> (StaticArrayId<D>, impl FnMut(&mut C, StaticArrayId<D>) -> (StaticArrayId<D>, StaticArrayId<D>)) 
    where D: Dimension + Copy, F: num::Float, C: Ctx2<Array<F, D>, Id = StaticArrayId<D>, Shape = <D as Dimension>::Pattern> 
{
    let id = mul_ctx_help(ctx, a, b);

    let f = move |ctx: &mut C, grad_id| {
        let da = mul_ctx_help(ctx, grad_id, b);
        let db = mul_ctx_help(ctx, grad_id, a);

        (da, db)
    };

    (id, f)
}


/// unfortunately, its still a bit more verbose than I like, but much better than the previous attempts.
/// At least now all the weights are easily accessible in a flattened representation
/// 
/// Or one could simply use a concrete type, and just use different names for different ids

struct CtxConcrete {}

impl CtxConcrete {
    fn store_weight<D>(&mut self, s: StaticArrayId<D>) { todo!() }
    fn store_grad<D, F>(&mut self, s: Array<F, D>) { todo!() }
    fn get<D: Dimension, F>(&self, id: StaticArrayId<D>) -> &Array<F, D> { todo!() }
    fn alloc<D: Dimension, F>(&self, shape: <D as Dimension>::Pattern) -> (StaticArrayId<D>, &mut Array<F, D>) { todo!() }
    fn alloc_temp<D: Dimension, F>(&self, shape: <D as Dimension>::Pattern) -> (StaticArrayId<D>, &mut Array<F, D>) { todo!() }

    fn dyn_store_weight(&mut self, s: ArrayId) { todo!() }
    fn dyn_store_grad<F>(&mut self, s: ArrayD<F>) { todo!() }
    fn dyn_get<F>(&self, id: ArrayId) -> &ArrayD<F> { todo!() }
    fn dyn_alloc<F>(&self, shape: &[usize]) -> (ArrayId, &mut ArrayD<F>) { todo!() }
    fn dyn_alloc_temp<F>(&self, shape: &[usize]) -> (ArrayId, &mut ArrayD<F>) { todo!() }    
}

fn mul_ctx_help_concrete<D, F>(ctx: &mut CtxConcrete, a: StaticArrayId<D>, b: StaticArrayId<D>) -> StaticArrayId<D>
where D: Dimension + Copy, F: num::Float {
    let aref = ctx.get::<D, F>(a);
    let bref = ctx.get::<D, F>(b);

    let (id, result) = ctx.alloc(aref.dim());
    result.iter_mut().zip(aref.iter()).zip(bref.iter()).for_each(|((y, a), b)| {
        *y = *a * *b;
    });

    id
}

fn mul_ctx_concrete<D, F>(ctx: &mut CtxConcrete, a: StaticArrayId<D>, b: StaticArrayId<D>) 
    -> (StaticArrayId<D>, impl FnMut(&mut CtxConcrete, StaticArrayId<D>) -> (StaticArrayId<D>, StaticArrayId<D>)) 
    where D: Dimension + Copy, F: num::Float
{
    let id = mul_ctx_help_concrete::<D, F>(ctx, a, b);

    let f = move |ctx: &mut CtxConcrete, grad_id| {
        let da = mul_ctx_help_concrete::<D, F>(ctx, grad_id, b);
        let db = mul_ctx_help_concrete::<D, F>(ctx, grad_id, a);

        (da, db)
    };

    (id, f)
}

/// Anyways, the trait method is probably better, its just one more line in every trait bound
/// 
/// At the end of a step, we could use something like Query<(...)> -> Iter<(...)>, to get all types
/// of a certain nature, as done in ecs.
/// Like ctx.get_all::<(&Gradient<Array2<f32>>, &mut Weight<Array2<f32>>)>(). Which we could use to update the weights,

/// This method is much more manual than the previous, and implementors of kernels must keep in mind when to use
/// alloc, and when to use alloc_temp, additionally, they are responsible for call free_temp, if any temporary storage is used.
/// Additionally, memory usage is potential higher for longer periods of time, since it is unclear when to free each Id.
/// (making the context opportunistically free Ids pretty must makes this whole entreprise pointless, since it boils down to 
///  an Arc or shared reference at the end)


fn main() {

}
